{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from collections import deque\n",
    "import datetime\n",
    "import json\n",
    "import csv\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "global face_file_name\n",
    "image_height, image_width = 64, 64\n",
    "max_images_per_class = 8000\n",
    " \n",
    "dataset_directory = \"dataset\"\n",
    "\n",
    "classes_list = [\"Fighting\",\"Shooting\",\"RoadAccidents\",\"Robbery\",\"Abuse\",\"Arrest\",\"Arson\",\"Assault\",\"Burglary\",\"Explosion\",\"Normal\"]\n",
    "\n",
    " \n",
    "model_output_size = len(classes_list)\n",
    "print(model_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"./model_8000.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_states = {\n",
    "    \"Andhra Pradesh\": \"AP\",\n",
    "    \"Arunachal Pradesh\": \"AR\",\n",
    "    \"Assam\": \"AS\",\n",
    "    \"Bihar\": \"BR\",\n",
    "    \"Chhattisgarh\": \"CG\",\n",
    "    \"Goa\": \"GA\",\n",
    "    \"Gujarat\": \"GJ\",\n",
    "    \"Haryana\": \"HR\",\n",
    "    \"Himachal Pradesh\": \"HP\",\n",
    "    \"Jharkhand\": \"JH\",\n",
    "    \"Karnataka\": \"KA\",\n",
    "    \"Kerala\": \"KL\",\n",
    "    \"Madhya Pradesh\": \"MP\",\n",
    "    \"Maharashtra\": \"MH\",\n",
    "    \"Manipur\": \"MN\",\n",
    "    \"Meghalaya\": \"ML\",\n",
    "    \"Mizoram\": \"MZ\",\n",
    "    \"Nagaland\": \"NL\",\n",
    "    \"Odisha\": \"OD\",\n",
    "    \"Punjab\": \"PB\",\n",
    "    \"Rajasthan\": \"RJ\",\n",
    "    \"Sikkim\": \"SK\",\n",
    "    \"Tamil Nadu\": \"TN\",\n",
    "    \"Telangana\": \"TS\",\n",
    "    \"Tripura\": \"TR\",\n",
    "    \"Uttar Pradesh\": \"UP\",\n",
    "    \"Uttarakhand\": \"UK\",\n",
    "    \"West Bengal\": \"WB\",\n",
    "    \"Na\": \"NA\"\n",
    "}\n",
    "\n",
    "# # Example: Accessing the short form of a state\n",
    "# print(indian_states[\"Tamil Nadu\"])  # Output: \"TN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id(directory,state_name):\n",
    "    id_count = len(os.listdir(directory))\n",
    "    input_string = f\"{state_name}\"\n",
    "    capitalized_string = input_string.title()\n",
    "    print(capitalized_string)\n",
    "    # print(capitalized_string)  # Output: \"Hello World\"\n",
    "\n",
    "    id_number = indian_states[f\"{capitalized_string}\"]\n",
    "    final_id_number = (f\"{id_number}{id_count}\")\n",
    "    \n",
    "    return final_id_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale(image):\n",
    "\tsr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "\tscale = 2\n",
    "\tmodelPath = r\"EDSR_x4.pb\"\n",
    "\tmethod = \"EDSR\"\n",
    "\tsr.readModel(modelPath)\n",
    "\tsr.setModel(method.lower(), scale) \n",
    "\n",
    "\t# Upscale the input image.\n",
    "\tresult = sr.upsample(image) \n",
    "\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_faces_from_stream(frame):\n",
    "#     try:\n",
    "#         print(\"calling extract faces 1\")\n",
    "#         model_path = \"weights.caffemodel\"  # Replace with the path to your model\n",
    "#         proto_path = os.path.join(model_path, \"deploy.prototxt.txt\")\n",
    "#         weight_path = os.path.join(model_path, \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "#         net = cv2.dnn.readNetFromCaffe(\"deploy_prototxt.txt\",\"weights.caffemodel\")\n",
    "\n",
    "#         # Convert the image to a numpy array\n",
    "#         # image = cv2.imdecode(np.frombuffer(image_stream.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "#         # image = image_stream\n",
    "\n",
    "#         # Get the dimensions of the image\n",
    "#         (h, w) = frame.shape[:2]\n",
    "\n",
    "#         # Preprocess the image for face detection\n",
    "#         blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104, 177, 123))\n",
    "\n",
    "#         # Set the input to the pre-trained model and perform face detection\n",
    "#         net.setInput(blob)\n",
    "#         detections = net.forward()\n",
    "\n",
    "#         faces = []\n",
    "#         for i in range(0, detections.shape[2]):\n",
    "#             confidence = detections[0, 0, i, 2]\n",
    "\n",
    "#             # Filter out weak detections\n",
    "#             if confidence > 0.2:\n",
    "#                 # Get the coordinates of the bounding box\n",
    "#                 box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "#                 (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "#                 # Crop the face from the image\n",
    "#                 face = frame[startY:endY, startX:endX]\n",
    "#                 face = upscale(face)\n",
    "#                 faces.append(face)\n",
    "#         # print(\"len of faces at extract 1 is\")\n",
    "#             # cv2.imwrite(f\"{i}_up.jpg\",face)\n",
    "#         print(\"len of faces at extract 1 is\",len(faces))\n",
    "#         len_faces = len(faces)\n",
    "#     except:\n",
    "#         pass\n",
    "#         len_faces = 0\n",
    "#         print(\"running except\")\n",
    "\n",
    "#     return len_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_from_stream(frame):\n",
    "    # try:\n",
    "    print(\"calling extract faces 1\")\n",
    "    model_path = \"weights.caffemodel\"  # Replace with the path to your model\n",
    "    proto_path = os.path.join(model_path, \"deploy.prototxt.txt\")\n",
    "    weight_path = os.path.join(model_path, \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "    net = cv2.dnn.readNetFromCaffe(\"deploy_prototxt.txt\",\"weights.caffemodel\")\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    # image = cv2.imdecode(np.frombuffer(image_stream.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "    # image = image_stream\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the image for face detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (h,w), (104, 177, 123))\n",
    "\n",
    "    # Set the input to the pre-trained model and perform face detection\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    faces = []\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # Filter out weak detections\n",
    "        if confidence > 0.2:\n",
    "            # Get the coordinates of the bounding box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Crop the face from the image\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            # face = upscale(face)\n",
    "            faces.append(face)\n",
    "    # print(\"len of faces at extract 1 is\")\n",
    "        # cv2.imwrite(f\"{i}_up.jpg\",face)\n",
    "    print(\"len of faces at extract 1 is\",len(faces))\n",
    "    len_faces = len(faces)\n",
    "    # except:\n",
    "        \n",
    "    #     len_faces = 0\n",
    "    #     print(\"running except\")\n",
    "\n",
    "    return len_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# frame = cv2.imread(r\"C:\\Users\\YASH\\Downloads\\istockphoto-1480574526-170667a.jpg\")\n",
    "# extract_faces_from_stream(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_faces_from_stream2(frame,filename):\n",
    "#     print(\"hello\")\n",
    "#     try:\n",
    "#         photo_loc = []\n",
    "#         frame = cv2.resize(frame,(240,320))\n",
    "#         model_path = \"weights.caffemodel\"  # Replace with the path to your model\n",
    "#         proto_path = os.path.join(model_path, \"deploy.prototxt.txt\")\n",
    "#         weight_path = os.path.join(model_path, \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "#         net = cv2.dnn.readNetFromCaffe(\"deploy_prototxt.txt\",\"weights.caffemodel\")\n",
    "\n",
    "#         # Convert the image to a numpy array\n",
    "#         # image = cv2.imdecode(np.frombuffer(image_stream.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "#         # image = image_stream\n",
    "\n",
    "#         # Get the dimensions of the image\n",
    "#         print(frame.shape)\n",
    "#         (h, w) = frame.shape[:2]\n",
    "\n",
    "#         # Preprocess the image for face detection\n",
    "#         blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104, 177, 123))\n",
    "\n",
    "#         # Set the input to the pre-trained model and perform face detection\n",
    "#         net.setInput(blob)\n",
    "#         detections = net.forward()\n",
    "\n",
    "#         faces = []\n",
    "#         for i in range(0, detections.shape[2]):\n",
    "#             confidence = detections[0, 0, i, 2]\n",
    "\n",
    "#             # Filter out weak detections\n",
    "#             if confidence > 0.1:\n",
    "#                 # Get the coordinates of the bounding box\n",
    "#                 box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "#                 (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "#                 # Crop the face from the image\n",
    "#                 face = frame[startY-10:endY+3, startX-5:endX+3]\n",
    "#                 # face = upscale(face)\n",
    "#                 faces.append(face)\n",
    "                \n",
    "#                 save_file_name = \"./face_dataset/\"+ str(filename) + str(i) + \".jpg\"\n",
    "#                 # save_file = f\"C:/Users/rudra/Downloads/VigilAI (2)/VigilAI{save_file_name[1::]}\"\n",
    "#                 photo_loc.append(save_file_name)\n",
    "#                 cv2.imwrite(save_file_name,face)\n",
    "#         # photo_loc = []\n",
    "#         print(\"length of faces is \",len(faces))\n",
    "#         len_faces = len(faces)\n",
    "#         if len(faces) == 0:\n",
    "#             photo_loc = ['./0_up.jpg']\n",
    "#     except:\n",
    "#         len_faces = 0\n",
    "#         photo_loc = ['./0_up.jpg']\n",
    "\n",
    "#     return len_faces,photo_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_from_stream2(frameMax,filename,max_index):\n",
    "    print(\"hello\")\n",
    "    # try:\n",
    "    photo_loc = []\n",
    "    frame = cv2.resize(frameMax,(240,320))\n",
    "    model_path = \"weights.caffemodel\"  # Replace with the path to your model\n",
    "    proto_path = os.path.join(model_path, \"deploy.prototxt.txt\")\n",
    "    weight_path = os.path.join(model_path, \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "    net = cv2.dnn.readNetFromCaffe(\"deploy_prototxt.txt\",\"weights.caffemodel\")\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    # image = cv2.imdecode(np.frombuffer(image_stream.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "    # image = image_stream\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    print(frame.shape)\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the image for face detection\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (h,w), (104, 177, 123))\n",
    "\n",
    "    # Set the input to the pre-trained model and perform face detection\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    faces = []\n",
    "    video_folder = filename.split(\".\")[0]\n",
    "    curr_dir =os.getcwd()\n",
    "    video_folder_path = os.path.join(curr_dir,\"face_dataset\",video_folder)\n",
    "    os.makedirs(video_folder_path,exist_ok=True)\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # Filter out weak detections\n",
    "        if confidence > 0.1:\n",
    "            # Get the coordinates of the bounding box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Crop the face from the image\n",
    "            face = frame[startY-10:endY+3, startX-5:endX+3]\n",
    "            # face = upscale(face)\n",
    "            faces.append(face)\n",
    "            \n",
    "            save_file_name = \"./face_dataset/\"+ str(filename) + str(i) + \".jpg\"\n",
    "            # save_file = f\"C:/Users/rudra/Downloads/VigilAI (2)/VigilAI{save_file_name[1::]}\"\n",
    "            photo_loc.append(save_file_name)\n",
    "            # cv2.imwrite(save_file_name,face)\n",
    "            # curr_dir = os.getcwd()\n",
    "            # filename_folder = os.makedirs(filename.split(\".\")[0],exist_ok=True)\n",
    "            # filename_folder_joined = os.path.join(curr_dir,\"face_dataset\",filename_folder,filename)\n",
    "            # video_folder = filename.split(\".\")[0]\n",
    "            # video_folder_path = os.path.join(curr_dir,\"face_dataset\",video_folder)\n",
    "            # os.makedirs(video_folder_path,exist_ok=True)\n",
    "\n",
    "\n",
    "            max_index = max_index/15\n",
    "            if face is not None and not face.size == 0:\n",
    "                save_file_name_ = os.path.join(video_folder_path,f\"{max_index}.jpg\")\n",
    "            # save_file_name = os.path.join(\"face_dataset\", f\"{filename}{i}.jpg\")\n",
    "            # photo_loc.append(save_file_name)\n",
    "            # cv2.imwrite(save_file_name, face)\n",
    "                cv2.imwrite(save_file_name_,face)\n",
    "            else:\n",
    "                print(\"empty\")\n",
    "    # photo_loc = []\n",
    "    print(\"length of faces is \",len(faces))\n",
    "    len_faces = len(faces)\n",
    "        \n",
    "    # except:\n",
    "    #     print(\"except for extract 2\")\n",
    "    #     len_faces = 0\n",
    "    #     photo_loc = ['./0_up.jpg']\n",
    "\n",
    "    return len_faces,photo_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clip(frames):\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc('V','P','8','0')  \n",
    "    frame_rate=15\n",
    "    output_video = cv2.VideoWriter(\"./output_clip.webm\", fourcc, frame_rate, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        output_video.write(frame)\n",
    "    output_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location():\n",
    "    try:\n",
    "        # Create a geocoder instance\n",
    "        url='http://ipinfo.io/json'\n",
    "        response=urlopen(url)\n",
    "        # Get the current GPS location\n",
    "        data=json.load(response)\n",
    "        coordinates=data['loc']\n",
    "        city=data['city']\n",
    "        state = data['region']\n",
    "        return city,coordinates,state\n",
    "    except:\n",
    "        city,coordinates,state='NA','NA','NA'\n",
    "        return city,coordinates,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_and_time():\n",
    "    data = datetime.datetime.now()\n",
    "    data=str(data)\n",
    "    date,time=data.split(' ')\n",
    "    date=date.replace('-','_')\n",
    "    time=time.replace(':','_')\n",
    "    time=time.replace('.','_')\n",
    "    return str(date),str(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_database(crime_type,frames):\n",
    "#     global face_file_name\n",
    "#     # Specify the current file path and the new file name\n",
    "#     current_file_path = \"./healthcare/healthcare/Eyebase/output_clip.webm\"\n",
    "#     city,coordinates,state=get_location()\n",
    "\n",
    "#     date,time=get_date_and_time()\n",
    "\n",
    "#     new_file_name =str(city)+'_'+date+'+'+time+'.webm'\n",
    "#     face_file_name = new_file_name\n",
    "#     file_path='Eyebase/'+new_file_name\n",
    "\n",
    "#     frames_in_videos = len(frames)\n",
    "#     counter = 0\n",
    "#     frames_in_videos = frames_in_videos//2\n",
    "#     frame_list = []\n",
    "#     frames_list = []\n",
    "#     print(frames_in_videos)\n",
    "\n",
    "#     for i,frame in enumerate(frames):\n",
    "#         if i > frames_in_videos - 2 and  i < frames_in_videos + 2:\n",
    "#                 number_of_faces = (extract_faces_from_stream(frame))\n",
    "#                 # number_of_faces2 = len(number_of_faces)\n",
    "#                 frame_list.append(number_of_faces)\n",
    "#                 frames_list.append(frame)\n",
    "#                 print(number_of_faces)\n",
    "    \n",
    "#     frame_list = np.array(frame_list)\n",
    "#     max_index = np.argmax(frame_list)\n",
    "#     print(\"max index is\",max_index)\n",
    "#     number_of_faces,photo_loc = (extract_faces_from_stream2(frames_list[max_index],new_file_name))\n",
    "\n",
    "#     count_dir= \"./healthcare/healthcare/Eyebase\"\n",
    "#     Fir_ID = id(count_dir,state)\n",
    "#     new_data = [file_path,crime_type,city,coordinates,date,time,photo_loc,Fir_ID]\n",
    "\n",
    "#     csv_file = \"./healthcare/healthcare/AIbase.csv\"\n",
    "\n",
    "#     with open(csv_file, 'a', newline='',encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         # Append the new data to the CSV file\n",
    "#         writer.writerow(new_data)\n",
    "\n",
    "#     # Extract the directory and the extension from the current file path\n",
    "#     directory = os.path.dirname(current_file_path)\n",
    "#     extension = os.path.splitext(current_file_path)[1]\n",
    "\n",
    "#     # Create the new file path by combining the directory, new file name, and extension\n",
    "#     new_file_path = os.path.join(directory, new_file_name)\n",
    "\n",
    "#     # Rename the file\n",
    "#     os.rename(current_file_path, new_file_path)\n",
    "#     print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "\n",
    "async def send_video(path):\n",
    "    video_filename = path \n",
    "    chunk_size = 1024*500\n",
    "    \n",
    "    async with websockets.connect('ws://192.168.25.171:8765') as websocket:  \n",
    "        with open(video_filename, 'rb') as video_file:\n",
    "            while True:\n",
    "                chunk = video_file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                await websocket.send(chunk)\n",
    "        print(\"sent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(crime_type,frames):\n",
    "    global face_file_name\n",
    "    # Specify the current file path and the new file name\n",
    "    current_file_path = \"./healthcare/healthcare/Eyebase/output_clip.webm\"\n",
    "    city,coordinates,state=get_location()\n",
    "\n",
    "    date,time=get_date_and_time()\n",
    "\n",
    "    new_file_name =str(city)+'_'+date+'+'+time+'.webm'\n",
    "    face_file_name = new_file_name\n",
    "    file_path='Eyebase/'+new_file_name\n",
    "\n",
    "    frames_in_videos = len(frames)\n",
    "    counter = 0\n",
    "    frames_in_videos = frames_in_videos//2\n",
    "    frame_list = []\n",
    "    frames_list = []\n",
    "    print(frames_in_videos)\n",
    "\n",
    "    # for i,frame in enumerate(frames):\n",
    "    #     if i > frames_in_videos - 5 and  i < frames_in_videos + 5:\n",
    "    #             number_of_faces = (extract_faces_from_stream(frame))\n",
    "    #             # number_of_faces2 = len(number_of_faces)\n",
    "    #             frame_list.append(number_of_faces)\n",
    "    #             frames_list.append(frame)\n",
    "    #             print(number_of_faces)\n",
    "    for frame in frames:\n",
    "        number_of_faces = extract_faces_from_stream(frame)\n",
    "        frame_list.append(number_of_faces)\n",
    "\n",
    "        \n",
    "    \n",
    "    frame_list = np.array(frame_list)\n",
    "    print(\"frame list is\",frame_list)\n",
    "    max_index = np.argmax(frame_list)\n",
    "    print(\"max index is\",max_index)\n",
    "    number_of_faces,photo_loc = extract_faces_from_stream2(frames[max_index],new_file_name,max_index)\n",
    "\n",
    "    count_dir= \"./healthcare/healthcare/Eyebase\"\n",
    "    Fir_ID = id(count_dir,state)\n",
    "    new_data = [file_path,crime_type,city,coordinates,date,time,photo_loc,Fir_ID]\n",
    "\n",
    "    csv_file = \"./healthcare/healthcare/AIbase.csv\"\n",
    "\n",
    "    with open(csv_file, 'a', newline='',encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Append the new data to the CSV file\n",
    "        writer.writerow(new_data)\n",
    "\n",
    "    # Extract the directory and the extension from the current file path\n",
    "    directory = os.path.dirname(current_file_path)\n",
    "    extension = os.path.splitext(current_file_path)[1]\n",
    "\n",
    "    # Create the new file path by combining the directory, new file name, and extension\n",
    "    new_file_path = os.path.join(directory, new_file_name)\n",
    "\n",
    "    # Rename the file\n",
    "    os.rename(current_file_path, new_file_path)\n",
    "    path = new_file_path\n",
    "    asyncio.get_event_loop().run_until_complete(send_video(path))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_database(crime_type,frames):\n",
    "    # Specify the source file path and the destination folder path\n",
    "    source_file = \"./output_clip.webm\"\n",
    "    destination_folder = \"./healthcare/healthcare/Eyebase\"\n",
    "\n",
    "    # Copy the file to the destination folder\n",
    "    shutil.copy(source_file, destination_folder)\n",
    "    create_database(crime_type,frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_on_live_video(video_file_path, output_file_path, window_size):\n",
    "#     frame_count=0\n",
    "#     frames=[]\n",
    "#     crime_type='NA'\n",
    "#     counter=0\n",
    "#     # Initialize a Deque Object with a fixed size which will be used to implement moving/rolling average functionality.\n",
    "#     predicted_labels_probabilities_deque = deque(maxlen = window_size)\n",
    " \n",
    "#     # Reading the Video File using the VideoCapture Object\n",
    "# #     video_reader = cv2.VideoCapture(video_file_path)\n",
    "#     video_reader = cv2.VideoCapture(video_file_path)\n",
    " \n",
    "#     # Getting the width and height of the video \n",
    "#     original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    " \n",
    "#     # Writing the Overlayed Video Files Using the VideoWriter Object\n",
    "#     # video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'VP80'), 24, (original_video_width, original_video_height))\n",
    " \n",
    "#     while True: \n",
    " \n",
    "#         # Reading The Frame\n",
    "#         status, frame = video_reader.read() \n",
    " \n",
    "#         if not status:\n",
    "#             break\n",
    " \n",
    "#         # Resize the Frame to fixed Dimensions\n",
    "#         resized_frame = cv2.resize(frame, (image_height, image_width))\n",
    "         \n",
    "#         # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
    "#         normalized_frame = resized_frame / 255\n",
    " \n",
    "#         # Passing the Image Normalized Frame to the model and receiving Predicted Probabilities.\n",
    "#         predicted_labels_probabilities = model.predict(np.expand_dims(normalized_frame, axis = 0))[0]\n",
    " \n",
    "#         # Appending predicted label probabilities to the deque object\n",
    "#         predicted_labels_probabilities_deque.append(predicted_labels_probabilities)\n",
    " \n",
    "#         # Assuring that the Deque is completely filled before starting the averaging process\n",
    "#         if len(predicted_labels_probabilities_deque) == window_size:\n",
    " \n",
    "#             # Converting Predicted Labels Probabilities Deque into Numpy array\n",
    "#             predicted_labels_probabilities_np = np.array(predicted_labels_probabilities_deque)\n",
    " \n",
    "#             # Calculating Average of Predicted Labels Probabilities Column Wise \n",
    "#             predicted_labels_probabilities_averaged = predicted_labels_probabilities_np.mean(axis = 0)\n",
    " \n",
    "#             # Converting the predicted probabilities into labels by returning the index of the maximum value.\n",
    "#             predicted_max=np.max(predicted_labels_probabilities_averaged)\n",
    "\n",
    "#             predicted_label = np.argmax(predicted_labels_probabilities_averaged)\n",
    "\n",
    "#             if predicted_max>0.6 and predicted_label<10:\n",
    "#                 predicted_label = np.argmax(predicted_labels_probabilities_averaged)\n",
    "#                 frame_count=frame_count+1\n",
    "#                 if frame_count<450:\n",
    "#                     if counter==0:\n",
    "#                         crime_type=classes_list[predicted_label]\n",
    "#                         counter=1\n",
    "#                     frames.append(frame)\n",
    "#                     save_clip(frames)\n",
    "#                 else:\n",
    "#                     if len(frames)>100:    \n",
    "#                         frame_count=0\n",
    "#                         frames=[]\n",
    "#             else:\n",
    "#                 predicted_label=10 \n",
    "\n",
    "                \n",
    "#             # Accessing The Class Name using predicted label.\n",
    "#             predicted_class_name = classes_list[predicted_label]\n",
    "\n",
    "           \n",
    "#         # Writing The Frame\n",
    "#         # video_writer.write(frame)\n",
    " \n",
    " \n",
    "#         cv2.imshow('CCTV FOOTAGE', frame)\n",
    " \n",
    "#         key_pressed = cv2.waitKey(1)\n",
    " \n",
    "#         if key_pressed == ord('q'):\n",
    "#             break\n",
    "            \n",
    "#     add_to_database(crime_type)\n",
    "#     cv2.destroyAllWindows()\n",
    "     \n",
    "#     # Closing the VideoCapture and VideoWriter objects and releasing all resources held by them. \n",
    "#     video_reader.release()\n",
    "#     # video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count=0\n",
    "import threading\n",
    "# frame_save = []\n",
    "\n",
    "class test:\n",
    "    def __init__ (self,video_file_path):\n",
    "        self.frame_count = 0 \n",
    "        self.frames = []\n",
    "        self.status=0\n",
    "        self.crime_type = \"NA\"\n",
    "        self.counter = 0\n",
    "        self.video_file_path=video_file_path\n",
    "        self.predicted_labels_probabilities_deque = deque(maxlen = 30)\n",
    "        self.video_reader = cv2.VideoCapture(self.video_file_path)\n",
    "        self.processing_thread = None\n",
    "        self.is_processing = False\n",
    "        self.window_size=30\n",
    "        self.ti=None\n",
    "        self.count=0\n",
    "        self.lock=threading.Lock()\n",
    "        self.t3=None\n",
    "        self.start_time = time.time()\n",
    "        self.end_time = 0\n",
    "\n",
    "\n",
    "    def start_processing(self):\n",
    "        print(self.video_file_path)\n",
    "        self.is_processing = True\n",
    "        print(\"gbg\")\n",
    "        # video_reader = cv2.VideoCapture(self.video_file_path)\n",
    "        self.processing_thread = threading.Thread(target=self.process_video)\n",
    "        self.processing_thread.start()\n",
    "        print(self.video_file_path)\n",
    "        # print(video_reader)\n",
    "        # self.status,self.frames = video_reader.read()\n",
    "        print(self.status)\n",
    "        # return frames\n",
    "\n",
    "    def stop_processing(self):\n",
    "        self.is_processing = False\n",
    "        if self.processing_thread is not None:\n",
    "            # self.processing_thread.join()\n",
    "            pass\n",
    "    \n",
    "    def process_video(self):\n",
    "        \n",
    "        # original_video_width = int(self.frames(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        # original_video_height = int(self.frames(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        print(\"kkk\")\n",
    "        counter=0\n",
    "        while self.is_processing:\n",
    "            self.lock.acquire()\n",
    "            print(\"lll\")\n",
    "            frames=self.frames\n",
    "            status, frame = self.video_reader.read()\n",
    "            if not status:\n",
    "                print(self.status)\n",
    "                break\n",
    "            print(\"mmm\")\n",
    "            resized_frame = cv2.resize(frame, (image_height, image_width))\n",
    "            normalized_frame = resized_frame / 255\n",
    "            # self.processing_thread.join()\n",
    "            self.lock.release()\n",
    "            self.ti=threading.Thread(target=test.predict_on_live_video,args= [self,normalized_frame,frame,counter])\n",
    "            self.ti.start()\n",
    "\n",
    "            # ti.join()\n",
    "            # predicted_labels_probabilities = model.predict(np.expand_dims(normalized_frame, axis = 0))[0]\n",
    "            # self.predicted_labels_probabilities_deque.append(predicted_labels_probabilities)\n",
    "            # if len(self.predicted_labels_probabilities_deque) == self.window_size:\n",
    "            #     predicted_labels_probabilities_np = np.array(self.predicted_labels_probabilities_deque)\n",
    "            #     predicted_labels_probabilities_averaged = predicted_labels_probabilities_np.mean(axis = 0)\n",
    "            #     predicted_max=np.max(predicted_labels_probabilities_averaged)\n",
    "            #     predicted_label = np.argmax(predicted_labels_probabilities_averaged)\n",
    "            #     if predicted_max>0.6 and predicted_label<10:\n",
    "            #         predicted_label = np.argmax(predicted_labels_probabilities_averaged)\n",
    "            #         self.frame_count = self.frame_count + 1\n",
    "            #         if frame_count<450:\n",
    "            #             if counter == 0:\n",
    "            #                 crime_type = classes_list[predicted_label]\n",
    "            #                 counter = 1\n",
    "            #             frames.append(frame)\n",
    "            #             save_clip(frames)\n",
    "            #         else:\n",
    "            #             if len(frames)>100:\n",
    "            #                 frame_count = 0\n",
    "            #                 frames = []\n",
    "            #     else:\n",
    "            #         predicted_label= 10\n",
    "            #     predicted_class_name = classes_list[predicted_label]\n",
    "            cv2.imshow('CCTV FOOTAGE',frame)\n",
    "            key_pressed = cv2.waitKey(1)\n",
    "            if key_pressed == ord('q'):\n",
    "                break\n",
    "        cv2.destroyAllWindows()\n",
    "        self.t3 = threading.Thread(target=save_clip,args=[self.frames])\n",
    "        self.t3.start()\n",
    "        self.end_time = time.time()\n",
    "        print(\"time==\",self.end_time-self.start_time)\n",
    "        time.sleep(3)\n",
    "        add_to_database(self.crime_type,self.frames)\n",
    "        # self.t3.join()\n",
    "        print(threading.active_count())\n",
    "            # video_reader.release()\n",
    "        # t1 = threading.Thread(target=read_video,args=[])\n",
    "        # t2 = threading.Thread(target=process_video,args=[])\n",
    "        # t1.start()\n",
    "        # t2.start()\n",
    "        # t1.join()\n",
    "        # t2.join()\n",
    "    def predict_on_live_video(self,normalized_frame,frame,counter,window_size=30):\n",
    "        # t3 = threading.Thread(target=save_clip,args=[self.frames])\n",
    "        self.lock.acquire()\n",
    "        # predicted_labels_probabilities_deque = deque(maxlen = window_size\n",
    "        # print(\"predict\",threading.active_count())\n",
    "        predicted_labels_probabilities = model.predict(np.expand_dims(normalized_frame, axis = 0))[0]\n",
    "        self.predicted_labels_probabilities_deque.append(predicted_labels_probabilities)\n",
    "        if len(self.predicted_labels_probabilities_deque) == self.window_size:\n",
    "                predicted_labels_probabilities_np = np.array(self.predicted_labels_probabilities_deque)\n",
    "                predicted_labels_probabilities_averaged = predicted_labels_probabilities_np.mean(axis = 0)\n",
    "                predicted_max=np.max(predicted_labels_probabilities_averaged)\n",
    "                predicted_label = np.argmax(predicted_labels_probabilities_averaged)\n",
    "                if predicted_max>0.6 and predicted_label<10:\n",
    "                    predicted_label = np.argmax(predicted_labels_probabilities_averaged)\n",
    "                    self.frame_count = self.frame_count + 1\n",
    "                    print(\"counter\",self.counter)\n",
    "                    if self.frame_count<450:\n",
    "                        print(\"frame_count\",self.frame_count)\n",
    "                        if self.counter == 0:\n",
    "                            print(\"ayaya?\")\n",
    "                            self.crime_type = classes_list[predicted_label]\n",
    "                            self.counter = 1\n",
    "                        self.frames.append(frame)\n",
    "                        # trying to save clip in a new thread\n",
    "                        # save_clip(self.frames)\n",
    "                        frame_save = self.frames\n",
    "                        # t3.start()\n",
    "                    else:\n",
    "                        if len(self.frames)>100:\n",
    "                            self.frame_count = 0\n",
    "                            # self.frames = []\n",
    "                else:\n",
    "                    self.frame_count = 0\n",
    "                    predicted_label= 10\n",
    "                predicted_class_name = classes_list[predicted_label]\n",
    "        else:\n",
    "            self.frames=[]\n",
    "        self.lock.release() \n",
    "        # self.ti.join()   \n",
    "        # self.count=1\n",
    "        # t2 = threading.Thread(target=test.process_video,args=[self])\n",
    "        # t1 = threading.Thread(target=test.read_video,args=[self])\n",
    "        # print(self.count)\n",
    "        # t2.start()\n",
    "        # t1.start()\n",
    "        # # t1.join()\n",
    "        # t2.join()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter.filedialog import askopenfilename\n",
    "# root = tk.Tk()\n",
    "# output_file_path = \"C:/Users/rudra/OneDrive/Desktop\"\n",
    "# window_size = 30\n",
    "# video_file_path = 0\n",
    "# def start():\n",
    "#     file_path = askopenfilename()\n",
    "#     output_file_path = \"C:/Users/rudra/OneDrive/Desktop\"\n",
    "#     window_size = 30\n",
    "#     print(file_path)\n",
    "#     video_file_path = file_path\n",
    "#     predict_on_live_video(video_file_path, output_file_path, window_size)\n",
    "# predict_on_live_video(video_file_path, output_file_path, window_size)\n",
    "# # start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter.filedialog import askopenfilename\n",
    "# def button_click():\n",
    "#     file_path = askopenfilename()\n",
    "\n",
    "#     # Display the selected file path\n",
    "#     # print(\"Selected file path:\", file_path)\n",
    "#     start(file_path)\n",
    "\n",
    "# # Create the Tkinter root window\n",
    "# button = tk.Button(root, text=\"Click Me!\",command = start)\n",
    "# button.pack()\n",
    "# root.mainloop()\n",
    "import tkinter as tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "root = tk.Tk()\n",
    "root.geometry(\"600x600\")\n",
    "# output_file_path = \"C:/Users/rudra/OneDrive/Desktop\"\n",
    "# window_size = 30\n",
    "# video_file_path = 0\n",
    "def start():\n",
    "    # print(\"tkinter\",threading.active_count())\n",
    "    file_path = askopenfilename()\n",
    "    output_file_path = \"C:/Users/dwive/Desktop\"\n",
    "    window_size = 30\n",
    "    print(file_path)\n",
    "    video_file_path = file_path\n",
    "    my_object = test(video_file_path)\n",
    "    # print(\"start\")\n",
    "    my_object.start_processing()\n",
    "def start_camera():\n",
    "    file_path = 0\n",
    "    output_file_path = \"C:/Users/dwive/Desktop\"\n",
    "    window_size = 30\n",
    "    print(file_path)\n",
    "    video_file_path = file_path\n",
    "    my_object = test(video_file_path)\n",
    "    # print(\"start\")\n",
    "    my_object.start_processing()\n",
    "\n",
    "# predict_on_live_video(video_file_path, output_file_path, window_size)\n",
    "# start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/VigilAI (2)/VigilAI/healthcare/healthcare/Eyebase/Lal Bahadur Nagar_2023_08_26+08_07_37_829842.webm\n",
      "D:/VigilAI (2)/VigilAI/healthcare/healthcare/Eyebase/Lal Bahadur Nagar_2023_08_26+08_07_37_829842.webm\n",
      "gbg\n",
      "kkk\n",
      "lll\n",
      "D:/VigilAI (2)/VigilAI/healthcare/healthcare/Eyebase/Lal Bahadur Nagar_2023_08_26+08_07_37_829842.webm\n",
      "0\n",
      "mmm\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "counter 0\n",
      "frame_count 1\n",
      "ayaya?\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "counter 1\n",
      "frame_count 2\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 3\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 4\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "counter 1\n",
      "frame_count 5\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "counter 1\n",
      "frame_count 6\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 7\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 8\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 9\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 10\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "counter 1\n",
      "frame_count 11\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 12\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 13\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "counter 1\n",
      "frame_count 14\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 15\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 16\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 17\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "counter 1\n",
      "frame_count 18\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 19\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "counter 1\n",
      "frame_count 20\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 21\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 22\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 23\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "counter 1\n",
      "frame_count 24\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "counter 1\n",
      "frame_count 25\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "counter 1\n",
      "frame_count 26\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 27\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "counter 1\n",
      "frame_count 28\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "counter 1\n",
      "frame_count 29\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "counter 1\n",
      "frame_count 30\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 31\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "counter 1\n",
      "frame_count 32\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 33\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "counter 1\n",
      "frame_count 34\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "counter 1\n",
      "frame_count 35\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "counter 1\n",
      "frame_count 36\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "counter 1\n",
      "frame_count 37\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 38\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "counter 1\n",
      "frame_count 39\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "counter 1\n",
      "frame_count 40\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "counter 1\n",
      "frame_count 41\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "counter 1\n",
      "frame_count 42\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "counter 1\n",
      "frame_count 43\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 44\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 45\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "counter 1\n",
      "frame_count 46\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "counter 1\n",
      "frame_count 47\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 48\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "counter 1\n",
      "frame_count 49\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 50\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "counter 1\n",
      "frame_count 51\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "counter 1\n",
      "frame_count 52\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "counter 1\n",
      "frame_count 53\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 54\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "counter 1\n",
      "frame_count 55\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 56\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "counter 1\n",
      "frame_count 57\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "counter 1\n",
      "frame_count 58\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 59\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "counter 1\n",
      "frame_count 60\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "counter 1\n",
      "frame_count 61\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "counter 1\n",
      "frame_count 62\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "counter 1\n",
      "frame_count 63\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "counter 1\n",
      "frame_count 64\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 65\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "counter 1\n",
      "frame_count 66\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "counter 1\n",
      "frame_count 67\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 68\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "counter 1\n",
      "frame_count 69\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "counter 1\n",
      "frame_count 70\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "counter 1\n",
      "frame_count 71\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "counter 1\n",
      "frame_count 72\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "counter 1\n",
      "frame_count 73\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "counter 1\n",
      "frame_count 74\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "counter 1\n",
      "frame_count 75\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "counter 1\n",
      "frame_count 76\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "counter 1\n",
      "frame_count 77\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "counter 1\n",
      "frame_count 78\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 79\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "counter 1\n",
      "frame_count 80\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "counter 1\n",
      "frame_count 81\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "counter 1\n",
      "frame_count 82\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 83\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "counter 1\n",
      "frame_count 84\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "counter 1\n",
      "frame_count 85\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "counter 1\n",
      "frame_count 86\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "counter 1\n",
      "frame_count 87\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "counter 1\n",
      "frame_count 88\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "counter 1\n",
      "frame_count 89\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "counter 1\n",
      "frame_count 90\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "counter 1\n",
      "frame_count 91\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "counter 1\n",
      "frame_count 92\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "counter 1\n",
      "frame_count 93\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "counter 1\n",
      "frame_count 94\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "counter 1\n",
      "frame_count 95\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "counter 1\n",
      "frame_count 96\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "counter 1\n",
      "frame_count 97\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "counter 1\n",
      "frame_count 98\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "counter 1\n",
      "frame_count 99\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "counter 1\n",
      "frame_count 100\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "counter 1\n",
      "frame_count 101\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "counter 1\n",
      "frame_count 102\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "counter 1\n",
      "frame_count 103\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "counter 1\n",
      "frame_count 104\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "counter 1\n",
      "frame_count 105\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "counter 1\n",
      "frame_count 106\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "counter 1\n",
      "frame_count 107\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "counter 1\n",
      "frame_count 108\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "counter 1\n",
      "frame_count 109\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "counter 1\n",
      "frame_count 110\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "counter 1\n",
      "frame_count 111\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "counter 1\n",
      "frame_count 112\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "counter 1\n",
      "frame_count 113\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "counter 1\n",
      "frame_count 114\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "counter 1\n",
      "frame_count 115\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "counter 1\n",
      "frame_count 116\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "counter 1\n",
      "frame_count 117\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "counter 1\n",
      "frame_count 118\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "counter 1\n",
      "frame_count 119\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "counter 1\n",
      "frame_count 120\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "counter 1\n",
      "frame_count 121\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "counter 1\n",
      "frame_count 122\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "counter 1\n",
      "frame_count 123\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "counter 1\n",
      "frame_count 124\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "counter 1\n",
      "frame_count 125\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "counter 1\n",
      "frame_count 126\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "counter 1\n",
      "frame_count 127\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "counter 1\n",
      "frame_count 128\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "counter 1\n",
      "frame_count 129\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "counter 1\n",
      "frame_count 130\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "counter 1\n",
      "frame_count 131\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "counter 1\n",
      "frame_count 132\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "counter 1\n",
      "frame_count 133\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "counter 1\n",
      "frame_count 134\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "counter 1\n",
      "frame_count 135\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "counter 1\n",
      "frame_count 136\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "counter 1\n",
      "frame_count 137\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "counter 1\n",
      "frame_count 138\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "counter 1\n",
      "frame_count 139\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "counter 1\n",
      "frame_count 140\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "counter 1\n",
      "frame_count 141\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "counter 1\n",
      "frame_count 142\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "counter 1\n",
      "frame_count 143\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "counter 1\n",
      "frame_count 144\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "counter 1\n",
      "frame_count 145\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "counter 1\n",
      "frame_count 146\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "counter 1\n",
      "frame_count 147\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "counter 1\n",
      "frame_count 148\n",
      "lll\n",
      "mmm\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "counter 1\n",
      "frame_count 149\n",
      "lll\n",
      "0\n",
      "time== 24.316490173339844\n",
      "74\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 0\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 1\n",
      "calling extract faces 1\n",
      "len of faces at extract 1 is 2\n",
      "frame list is [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 0 0 2 2 1 1 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 1 2 1 1 0 1 0 0 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 2 2 1 1 2 2 2 1 1 1 1 1\n",
      " 1 0 0 2 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 2 2 2 2 2 1 1 2 2 2 1 1 2\n",
      " 2 1 1 0 0 0 2 1 1 0 0 0 1 1 2 2 1 0 0 1 2 1 1 1 2 2 1 1 2 2 2 2 1 1 1 1 1\n",
      " 2]\n",
      "max index is 0\n",
      "hello\n",
      "(320, 240, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-589 (process_video):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YASH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\YASH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\YASH\\AppData\\Local\\Temp\\ipykernel_10336\\2032184301.py\", line 102, in process_video\n",
      "  File \"C:\\Users\\YASH\\AppData\\Local\\Temp\\ipykernel_10336\\3714409920.py\", line 8, in add_to_database\n",
      "  File \"C:\\Users\\YASH\\AppData\\Local\\Temp\\ipykernel_10336\\534466109.py\", line 60, in create_database\n",
      "  File \"c:\\Users\\YASH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 656, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'Thread-589 (process_video)'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "length of faces is  6\n",
      "Uttar Pradesh\n"
     ]
    }
   ],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter.filedialog import askopenfilename\n",
    "def button_click():\n",
    "    file_path = askopenfilename()\n",
    "\n",
    "    # Display the selected file path\n",
    "    # print(\"Selected file path:\", file_path)\n",
    "    start(file_path)\n",
    "\n",
    "# def cam():\n",
    "#     cam_start(0)\n",
    "\n",
    "# Create the Tkinter root window\n",
    "button = tk.Button(root, text=\"Click Me!\",command = start)\n",
    "button1 = tk.Button(root, text=\"Camera!\",command = start_camera)\n",
    "button.pack()\n",
    "button1.pack()\n",
    "root.mainloop()\n",
    "\n",
    "# # tkinter good button\n",
    "# button = tk.Button(root,font=\"comicsans 35 bold\",bg=\"yellow\",padx=30, text=\"Click Me!\",command = start)\n",
    "# button.place(x= 100,y=300)\n",
    "# button.pack(pady= 120)\n",
    "\n",
    "# button2 = tk.Button(root,font=\"comicsans 35 bold\",bg=\"yellow\",padx=30, text=\"Live Cam!\",command = cam_start)\n",
    "# button2.place(x=100,y=400)\n",
    "# button.pack(pady= 120)\n",
    "# root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
